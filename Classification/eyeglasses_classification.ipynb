{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GLQF8DFHBRQm"
   },
   "source": [
    "# Eyeglasses presence detection\n",
    "\n",
    "In this tutorial, we will create and train a simple model for predicting if a person on an image wears eyeglasses.\n",
    "\n",
    "We would like the model to be small and fast, so for this simple task, it would be a good decision to train a small model from scratch instead of using transfer learning from a heavy pretrained model.\n",
    "\n",
    "We will train our network on the famous CelebA dataset. It's a dataset of images with celebrities' faces. Every image is labeled with a set of attributes. One of them is the presence of eyeglasses, so the dataset suits perfectly for our task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WuQL59ZLQtA-"
   },
   "source": [
    "# Install libraries\n",
    "First, we need to prepare our work environment and install the necessary Python packages. If you're using Google Colab, you already have them, and you can skip next cell.\n",
    "\n",
    "We added strict version requirements for the packages for better reproducibility.\n",
    "\n",
    "Note that these versions of packages will replace already installed ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hvxJGrRUQp2x"
   },
   "outputs": [],
   "source": [
    "%pip install -q \\\n",
    "    numpy==1.18.2 torch==1.4.0 torchvision==0.5.0 \\\n",
    "    tqdm==4.43.0 pillow==7.0.0 matplotlib==3.2.0 \\\n",
    "    pandas==1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LMClrIYWRPaO"
   },
   "source": [
    "# Import libraries\n",
    "\n",
    "Let's import the libraries we will use in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2c2BMzpzRDRT"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from IPython.display import clear_output, display, HTML\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "pd.options.display.precision = 3\n",
    "pd.options.display.max_rows = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Qz3Cyl5i_w3k"
   },
   "source": [
    "Let's check the PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2UPy6A2_1yd"
   },
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zLpGQqLS_6k_"
   },
   "source": [
    "For the better reproducibility, it would be useful to set random seeds for the libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0AfbU2mPzip3"
   },
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Flip values for slower training speed, but more determenistic results.\n",
    "torch.backends.cudnn.deterministic = False\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCb3yeqLAgvu"
   },
   "source": [
    "It's recommended to train neural networks on GPU. However, it's possible to train them on a CPU as well. We will use the 0th GPU if a GPU is available and CPU otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3nIQc6GGAhDC"
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZtYLZb3-BKFq"
   },
   "source": [
    "Let's define some paths where we will store the images from the dataset and metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p8JKs6GVvwzG"
   },
   "outputs": [],
   "source": [
    "MAIN_ATTRIBUTE = 'Eyeglasses'                                  # Name of the class we will predict\n",
    "IMAGE_SIZE = 64                # Size of the model's input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGhhfO0sA2MY"
   },
   "outputs": [],
   "source": [
    "WORKING_DIR_PATH = Path('.')                                   # Base directory for all the content\n",
    "\n",
    "IMAGES_ZIP_PATH = WORKING_DIR_PATH / 'img_align_celeba.zip'    # Archive with images\n",
    "IMAGES_DIR_PATH = IMAGES_ZIP_PATH.with_suffix('')              # Directory with images\n",
    "ANNOTATIONS_PATH = WORKING_DIR_PATH / 'list_attr_celeba.txt'   # Text document with labels for every image.\n",
    "PARTITION_PATH = WORKING_DIR_PATH / 'list_eval_partition.txt'  # Text document with marks what subset\n",
    "                                                               # (train, validation or split) an image belongs to.\n",
    "\n",
    "CHECKPOINTS_PATH = WORKING_DIR_PATH / 'checkpoints'            # Path to the weights of the trained model\n",
    "ONNX_PATH = WORKING_DIR_PATH / 'classifier.onnx'\n",
    "\n",
    "CHECKPOINTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "CELEBA_FACE_SIZE = 178"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xYEzY0V09yfm"
   },
   "source": [
    "The following constants desctibe the parameters of our model and the training settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YCbp8uzBl2KV"
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 128               # Batch size\n",
    "EPOCHS = 20                    # Number of epochs of training\n",
    "\n",
    "LR = 1e-2                      # Learning rate\n",
    "LR_DECAY_STEP = 7              # Number of epochs after that we will decrease the learning rate\n",
    "LR_DECAY_GAMMA = 0.1           # Decaying coefficient\n",
    "WEIGHT_DECAY = 1e-4            # Weight decay coefficient\n",
    "\n",
    "CONSISTENCY_WEIGHT = 0.1       # Coefficient for the consistency part of the loss function\n",
    "\n",
    "NUM_WORKERS = 4                # Number of parallel processes for loading images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ctO8JKBV1OtS"
   },
   "source": [
    "# Load the CelebA dataset\n",
    "\n",
    "\n",
    "To train a model, you have to download images, annotations, and train/test partition lists from [the official CelebA Google Drive](https://drive.google.com/drive/folders/0B7EVK8r0v71pWEZsZE9oNnFzTm8). Unfortunately, we can't redistribute the dataset because of license restrictions.\n",
    "\n",
    "You need three files.\n",
    "\n",
    "- `img_align_celeba.zip` — an archive with cropped images,\n",
    "- `list_attr_celeba.txt` — a list of the image attributes,\n",
    "- `list_eval_partition.txt` — a file with dataset partition to three subsets.\n",
    "\n",
    "After downloading, just put them into the working directory you set above or upload them to the CelebA folder of your Google Drive if you are using Google Colab. You can copy the files from your Google Drive to Colab with the following function. For that, uncomment the last line and run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mKpIUPonQL1z"
   },
   "outputs": [],
   "source": [
    "def copy_files_from_drive():\n",
    "    DRIVE_PATH = WORKING_DIR_PATH / 'drive' / 'My Drive'  # The mounted Google Drive\n",
    "    from google.colab import drive\n",
    "    drive.mount(str(DRIVE_PATH.parent))\n",
    "    \n",
    "    shutil.copy(DRIVE_PATH /'CelebA'/ 'Img'/ IMAGES_ZIP_PATH.name, IMAGES_ZIP_PATH)\n",
    "    shutil.copy(DRIVE_PATH /'CelebA'/ 'Anno' /ANNOTATIONS_PATH.name, ANNOTATIONS_PATH)\n",
    "    shutil.copy(DRIVE_PATH /'CelebA'/ 'Eval'/ PARTITION_PATH.name, PARTITION_PATH)\n",
    "\n",
    "\n",
    "## Uncomment for copying files to Colab\n",
    "# copy_files_from_drive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r-USsrAsQL12"
   },
   "source": [
    "Now we can extract the images to a directory. If you encounter message \"Disk is almost full\" on Google Colab, please click the \"ignore\" button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BxWQ0HVFchrs",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not IMAGES_DIR_PATH.exists():\n",
    "    shutil.unpack_archive(str(IMAGES_ZIP_PATH), str(WORKING_DIR_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqOyQGgOXxTM"
   },
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tq9_4waTX0wT"
   },
   "source": [
    "Now let's load the image attributes. The format of the text document with the image attributes is quite simple. It's just a plain text file. The first line is a number of rows, the second one is a header and the others contain file names and attributes.\n",
    "\n",
    "We can load this file into a so-called dataframe with Pandas. There is a function `read_csv` that can load data from any text file with separators. In our case, we set the separator to a sequence of whitespaces (`\\s+`), ignore the first line with the number of rows (we actually don't need it) and use the first column as an index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dKqmlCyMsQ5k"
   },
   "outputs": [],
   "source": [
    "df_attr = pd.read_csv(ANNOTATIONS_PATH, sep='\\s+', skiprows=1, index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOC8JY2cZRbA"
   },
   "source": [
    "Jupyter and Google Colab can show the contents of the dataframe. For that just run a cell with its name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D9SEqEsivvJn"
   },
   "outputs": [],
   "source": [
    "df_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGwA-A8gZhGE"
   },
   "source": [
    "As you can see, the values are encoded with -1 and 1. For convenience let's recode them to a binary (0 and 1) representation. Also, we can leave only the column with the attribute we are going to train the classifier for. For the latter, we can just replace -1 with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sv5HUwSsZ7e9"
   },
   "outputs": [],
   "source": [
    "df_attr = df_attr[[MAIN_ATTRIBUTE]].replace({-1: 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DHSdbvcaCkGc"
   },
   "source": [
    "Now we have a dataframe with a single column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nNGR5DKgvvW7"
   },
   "outputs": [],
   "source": [
    "df_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B7o5B9FU0sVQ"
   },
   "source": [
    "# Exploratory data analysis\n",
    "\n",
    "The first step before model training is always exploring and analyzing the data. In this simple tutorial, we won't perform complex analysis but will take a look at the labels and determine if the dataset is balanced or not.\n",
    "\n",
    "Let's pick a set of random images, both with eyeglasses and without them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0BF8G97wVTRu"
   },
   "outputs": [],
   "source": [
    "samples_with_glasses = list(df_attr[df_attr[MAIN_ATTRIBUTE] == 1].sample(25, random_state=RANDOM_SEED).index)\n",
    "samples_without_glasses = list(df_attr[df_attr[MAIN_ATTRIBUTE] == 0].sample(25, random_state=RANDOM_SEED).index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4RmsHesUYQtw"
   },
   "source": [
    "For convenience, we can write a function for displaying a grid with images and titles above them. It will accept a list of image names and a list of titles that contains image names by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hNJs6zRiVnjv"
   },
   "outputs": [],
   "source": [
    "def load_and_show(image_names, titles=None, directory=IMAGES_DIR_PATH):\n",
    "    if titles is None:\n",
    "        titles = image_names\n",
    "    N = len(image_names)\n",
    "    cols = int(math.sqrt(N))\n",
    "    rows = int(math.ceil(N / cols))\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    image_names_iter = iter(image_names)\n",
    "    titles_iter = iter(titles)\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            try:\n",
    "                name = next(image_names_iter)\n",
    "                title = next(titles_iter)\n",
    "            except StopIteration:\n",
    "                plt.show()\n",
    "                return\n",
    "            plt.subplot(rows, cols, cols * r + c + 1)\n",
    "            with Image.open(Path(directory) / name) as image_pil:\n",
    "                image = np.array(image_pil.convert('RGB'))\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rgV4GVyeYpSx"
   },
   "source": [
    "Let's take a look at the images!\n",
    "\n",
    "With eyeglasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jLEkq3fXvvk7"
   },
   "outputs": [],
   "source": [
    "load_and_show(samples_with_glasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylOxYZuoC7sV"
   },
   "source": [
    "Without eyeglasses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xWjTbJGcX4NB"
   },
   "outputs": [],
   "source": [
    "load_and_show(samples_without_glasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GhOMFrUYYr9q"
   },
   "source": [
    "As we can see, the labels seem to be true.\n",
    "\n",
    "Now let's take a look at the percentage of people with eyeglasses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2HVY_FF6xThP"
   },
   "outputs": [],
   "source": [
    "ratio = df_attr[MAIN_ATTRIBUTE].mean()\n",
    "print(f'{MAIN_ATTRIBUTE}: {ratio * 100:0.2f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tlV6PxkzgxrG"
   },
   "source": [
    "As we can see, this attribute is unbalanced, so that we have to use one of the balancing techniques like undersampling, oversampling, weighting, or something else.\n",
    "Undersampling looks like the simplest solution for our case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LWDGfq4jh4Rx"
   },
   "source": [
    "Now let's load the list of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8nTDdNeyyttR"
   },
   "outputs": [],
   "source": [
    "df_partition = pd.read_csv(PARTITION_PATH, sep='\\s+', names=['fold'], index_col=0)\n",
    "df_partition.fold.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XTyARbgrh_vt"
   },
   "source": [
    "We can use the 0th fold as a train set and the union of the 1st and the 2nd folds as a validation set. We won't optimize hyperparameters in this small tutorial, so we don't need a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsCaBQwRVoNL"
   },
   "outputs": [],
   "source": [
    "df_attr = df_attr.join(df_partition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nR9NONRLy6cD"
   },
   "outputs": [],
   "source": [
    "df_train = df_attr[df_attr.fold == 0]\n",
    "df_valid = df_attr[df_attr.fold != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QPMswZSvKNsy"
   },
   "source": [
    "For undersampling, we can take an equal number of images with persons that wear and that don't wear eyeglasses. To do that, we will use the following handy function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qnOHXpRKB686"
   },
   "outputs": [],
   "source": [
    "def equalize(df, attribute):\n",
    "    N = len(df)\n",
    "    ones = df[attribute].sum()\n",
    "    k = min(ones, N - ones)\n",
    "    df_ones = df[df[attribute] == 1].sample(k)\n",
    "    df_zeros = df[df[attribute] == 0].sample(k)\n",
    "    index = list(df_ones.index) + list(df_zeros.index)\n",
    "    random.shuffle(index)\n",
    "    return df.loc[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uGJDGtrhKmyW"
   },
   "source": [
    "Now we can equalize the sizes of the subsets. If you like to train a model on the whole dataset just comment or skip the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqdRKzzNC8Vo"
   },
   "outputs": [],
   "source": [
    "df_train = equalize(df_train, MAIN_ATTRIBUTE)\n",
    "df_valid = equalize(df_valid, MAIN_ATTRIBUTE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-yLyqNUL05Ji"
   },
   "source": [
    "# Create dataloaders\n",
    "\n",
    "For loading the data, we have to create a dataset and a data loader for both of the folds.\n",
    "\n",
    "A dataset is an iterator-like object that returns an image and its label at every step. A loader is an object that aggregates the output of the dataset and returns batches.\n",
    "\n",
    "Let's start with image transformations that play an important role in the data loading process. They crop an original image to make it square, they also convert the image to a PyTorch tensor and apply normalization.\n",
    "\n",
    "In the transforms for the train set, we can add augmentations. We will use only horizontal flipping, random affine transformation and random color changing.\n",
    "\n",
    "Note that we don't normalize input images. By default `ToTensor()` rescales values to the range `[0, 1]`. Often for better convergence, it would be better to rescale pixel values to a different range. However, for our task, the default range is good enough.\n",
    "\n",
    "If you need to normalize pixel values (eg. for transfer learning), consider adding `transforms.Normalize(mean, std)` after `transforms.ToTensor()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqP2J56FR6HK"
   },
   "outputs": [],
   "source": [
    "crop_resize = transforms.Compose([\n",
    "    transforms.CenterCrop(CELEBA_FACE_SIZE),\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "])\n",
    "\n",
    "transforms_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.2, hue=0.1),\n",
    "    transforms.RandomAffine(3, scale=(0.95, 1.05)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "transforms_valid = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FoezjrzTbTtO"
   },
   "source": [
    "For faster data loading, we can prepare images and load them to memory. We will put all the cropped images to a list.\n",
    "\n",
    "If your dataset is big or you don't have enough amount of free memory, it might be a bad idea. In that case, you have to add images right in `Dataset` object or consider alternative ways to store your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_54xNqdabbVS"
   },
   "outputs": [],
   "source": [
    "def load_images(df):\n",
    "    images = []\n",
    "    for image_name in tqdm(df.index, dynamic_ncols=True, leave=False):\n",
    "        image_pil = Image.open(IMAGES_DIR_PATH / image_name)\n",
    "        image = crop_resize(image_pil)\n",
    "        images.append(image)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WGfpWrnzc2FE"
   },
   "outputs": [],
   "source": [
    "images_train = load_images(df_train)\n",
    "images_valid = load_images(df_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Aw5YtXaJbSFE"
   },
   "source": [
    "The dataset object is pretty simple. It loads an image from disk and applies the transforms to it. It returns a transformed image and the corresponding label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9tvz4HaYlsq-"
   },
   "outputs": [],
   "source": [
    "class CelebaDataset(data.Dataset):\n",
    "    def __init__(self, df, images, transforms=None):\n",
    "        self.df = df\n",
    "        self.images = images\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.df.iloc[index]\n",
    "        image_name = row.name\n",
    "        attrs = row[MAIN_ATTRIBUTE]\n",
    "        image = self.images[index]\n",
    "        image_tensor = self.transforms(image)\n",
    "        return image_tensor, attrs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KurQ2e9UEc0r"
   },
   "source": [
    "For convenience, we can also write a function that performs a reverse transformation from normalized tensor to a NumPy array with pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REc5d-u8w6Ni"
   },
   "outputs": [],
   "source": [
    "def decode(tensor):\n",
    "    return (tensor.cpu()\n",
    "                  .clamp(0, 1)\n",
    "                  .numpy()\n",
    "                  .transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZCdVA3OZEqri"
   },
   "source": [
    "Let's create datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YoUEAjwek-s4"
   },
   "outputs": [],
   "source": [
    "dataset_train = CelebaDataset(df_train, images_train, transforms_train)\n",
    "dataset_valid = CelebaDataset(df_valid, images_valid, transforms_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xi35oAU0EtoC"
   },
   "source": [
    "Now we can check them by loading the first image from the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7RrVd0aeg35"
   },
   "outputs": [],
   "source": [
    "image, attrs = next(iter(dataset_valid))\n",
    "\n",
    "plt.imshow(decode(image))\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pN-5uFAoEs4I"
   },
   "source": [
    "Looks good enough. The image is small, and at the same time, it contains the details for eyeglasses detecting.\n",
    "\n",
    "Data loaders can also accept a weighted sampler. We can tune the weights so that the loader will load images with eyeglasses and without ones with approximately equal probability.\n",
    "\n",
    "Note that it's useless for a dataset that was already equalized, but we leave this code for the case when you need to train on the whole unbalanced dataset.\n",
    "\n",
    "For computing weights, we can write a function that multiplies the probability by the ratio of numbers of images with different labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qd5Tv1m2QL4P"
   },
   "outputs": [],
   "source": [
    "def get_weights(arr):\n",
    "    N = len(arr)\n",
    "    ones = arr.sum()\n",
    "    zeros = N - ones\n",
    "    return (1 - arr) + arr * zeros / ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y0lZFYHXQL4R"
   },
   "outputs": [],
   "source": [
    "weights_train = get_weights(df_train[MAIN_ATTRIBUTE].values)\n",
    "weights_valid = get_weights(df_valid[MAIN_ATTRIBUTE].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qnydXTQeQL4U"
   },
   "source": [
    "Now we can create the loaders. The train one will shuffle images and sample them with weights, the validation one will just return the images from the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Iu1lbd2hQL4V"
   },
   "outputs": [],
   "source": [
    "loader_train = data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    sampler=data.WeightedRandomSampler(weights_train, len(df_train)),\n",
    "    num_workers=NUM_WORKERS,\n",
    "    drop_last=True)\n",
    "loader_valid = data.DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    drop_last=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yCXG6T360z3n"
   },
   "source": [
    "# Create a model\n",
    "\n",
    "Now it's time to create a model. We will use separable convolution with batch normalization as the building blocks of our model. A separable convolution is a block with two convolutions instead of a single one: a depthwise one and a pointwise one. Such convolutions are faster and contain fewer parameters. The idea is borrowed from the paper [Howard A. G. et al. *MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications*](https://arxiv.org/abs/1704.04861)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eW_-K7IBln8-"
   },
   "outputs": [],
   "source": [
    "class SeparableConvBN(nn.Module):\n",
    "    def __init__(self, channels_in, channels_out):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.Sequential(\n",
    "            nn.Conv2d(channels_in, channels_in,\n",
    "                      kernel_size=3, padding=1,\n",
    "                      groups=channels_in, bias=False),\n",
    "            nn.BatchNorm2d(channels_in),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channels_in, channels_out,\n",
    "                      kernel_size=1, padding=0,\n",
    "                      bias=False),\n",
    "            nn.BatchNorm2d(channels_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t-pFePX9QL4b"
   },
   "source": [
    "Also, we will use a downsampling layer introduced in the paper [Zhang R. *Making Convolutional Networks Shift-Invariant Again*](https://richzhang.github.io/antialiased-cnns/). Antialiasing improves the stability of the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r0O68C4KQL4c"
   },
   "outputs": [],
   "source": [
    "class Downsample(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        \n",
    "        a = np.array([1., 2., 1.], dtype=np.float32)\n",
    "        a2 = a[:,None] * a[None,:]\n",
    "        filt = torch.tensor(a2 / a2.sum())[None,None,:,:].repeat((channels,1,1,1))\n",
    "        self.register_buffer('filt', filt)\n",
    "        self.pad = nn.ReflectionPad2d([1, 1, 1, 1])\n",
    "        self.channels = channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.conv2d(self.pad(x), self.filt, stride=2, groups=self.channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZCVNUrAGi8l"
   },
   "source": [
    "The model is a sequence of separable antialiased convolutions with a linear layer at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dNSQyTgDlZs-"
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        blocks = []\n",
    "\n",
    "        blocks.extend([ \n",
    "            # 64x64\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Downsample(16),\n",
    "\n",
    "            # 32x32\n",
    "            SeparableConvBN(16, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Downsample(32),\n",
    "            \n",
    "            # 16x16\n",
    "            SeparableConvBN(32, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Downsample(32),\n",
    "\n",
    "            # 8x8\n",
    "            SeparableConvBN(32, 32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Downsample(32),\n",
    "            \n",
    "            # 4x4\n",
    "            SeparableConvBN(32, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            Downsample(64),\n",
    "            \n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "\n",
    "            nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0),\n",
    "            nn.Flatten(1),\n",
    "        ])\n",
    "\n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.blocks(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uwk9G1bVHUms"
   },
   "source": [
    "Now we can create a model instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4RSzUPpjlrKv"
   },
   "outputs": [],
   "source": [
    "model = Classifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TFcwsG8P0IKj"
   },
   "source": [
    "# Train the model\n",
    "\n",
    "One of the most significant parts of the model training process is a loss function. For classification, the natural choice is binary cross-entropy.\n",
    "\n",
    "Also, we will add a so-called geometric consistency loss that demands the predictions after small shifts of flipping will become the same. We will minimize the mean squared error between two predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhyOIs9_6DBe"
   },
   "outputs": [],
   "source": [
    "bce = nn.BCEWithLogitsLoss()\n",
    "mse = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fAObsIJsH0En"
   },
   "source": [
    "To make the model much faster, let's move it to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IM7nxQqW-mva"
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zrwW7YGzH82y"
   },
   "source": [
    "For measuring the quality of the model, we will use the [F1 score](https://en.wikipedia.org/wiki/F1_score). It's a geometric mean of precision and recall. It works well even with unbalanced datasets.\n",
    "\n",
    "For computing F1, we will use the following function that accepts a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A2ROp_z_-o2C"
   },
   "outputs": [],
   "source": [
    "def f1_score(conf_mat):\n",
    "    tp = conf_mat[1, 1]\n",
    "    fp = conf_mat[1, 0]\n",
    "    fn = conf_mat[0, 1]\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7jnFojZuIjxM"
   },
   "source": [
    "For validation, we will use the following function. It computes the value of the loss function and metrics for the whole validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4o0Nn8YPDbKY"
   },
   "outputs": [],
   "source": [
    "def valid(loader=loader_valid):\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((2, 2), dtype=int)\n",
    "    loss = 0\n",
    "    pbar = tqdm(enumerate(loader), total=len(loader), dynamic_ncols=True, leave=False, desc='Validating')\n",
    "    for i, (images, attrs) in pbar:\n",
    "        images = images.to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(images)\n",
    "            loss += bce(outputs, attrs.unsqueeze(1).to(device).float()).cpu().item()\n",
    "        outputs = outputs.squeeze(1)\n",
    "        preds = (outputs.cpu() > 0).int()\n",
    "        for y_pred, y_true in zip(preds, attrs):\n",
    "            confusion_matrix[y_true.item()][y_pred.item()] += 1\n",
    "        precision, recall, f1 = f1_score(confusion_matrix)\n",
    "        pbar.set_postfix({\n",
    "            'Lclass': f'{loss / (i + 1):0.2f}',\n",
    "            'F1': f'{f1:0.2f}',\n",
    "            'Precision': f'{precision:0.2f}',\n",
    "            'Recall': f'{recall:0.2f}',\n",
    "        }, refresh=False)\n",
    "    return loss / len(loader), precision, recall, f1, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XwaekedsIuHi"
   },
   "source": [
    "Let's run it on an untrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3lCdf2EO-WWB"
   },
   "outputs": [],
   "source": [
    "loss, precision, recall, f1, confusion_matrix = valid()\n",
    "print(f'Loss: {loss:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ito80tbVIyKR"
   },
   "source": [
    "We got the value that is close to $\\ln 2 \\approx 0.693$ for the loss function. We use binary cross-entropy, and this value means that the probability of a correct answer for the untrained model is $\\frac12$.\n",
    "\n",
    "For optimization, we will use Adam. It's a well-known algorithm that is proved to be a good enough first choice.\n",
    "\n",
    "Also, we will use a scheduler that will reduce the learning rate periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PmhyrMu2vJq7"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_DECAY_STEP, gamma=LR_DECAY_GAMMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tycMFZkudD2Z"
   },
   "source": [
    "For random changing the batch, we will shift every image in the batch for one pixel to a random direction or alternatively flip images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ma7PmWWEQL41"
   },
   "outputs": [],
   "source": [
    "def random_change(images):\n",
    "    pad = nn.ReflectionPad2d(1)\n",
    "    images_padded = pad(images)\n",
    "    outputs = []\n",
    "    for i in range(images.size(0)):\n",
    "        dx = random.randint(-1, 1)\n",
    "        dy = random.randint(-1, 1)\n",
    "        if dx == 0 and dy == 0:\n",
    "            outputs.append(torch.flip(images[[i]], [-1]))\n",
    "        else:\n",
    "            h, w = image.shape[-2:]\n",
    "            outputs.append(images_padded[[i], :, 1+dy:h+dy+1, 1+dx:w+dx+1])\n",
    "    return torch.cat(outputs, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JG7g7EFRQL43"
   },
   "source": [
    "To get smoother values for the loss function, we will compute an exponential moving average. `alpha` is a smoothing coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7lV4cPHXQL43"
   },
   "outputs": [],
   "source": [
    "def ema(new, old, alpha=0.6):\n",
    "    if old is None or math.isnan(old):\n",
    "        return new\n",
    "    return alpha * new + (1 - alpha) * old"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Pvw9xJBGQL44"
   },
   "source": [
    "It's time to write the main training loop. There are actually two loops, one for epochs and another for batches. We will perform a validation every epoch.\n",
    "\n",
    "We will store running statistics to the `stats` dataframe. We will be able to use it later to take a look at the training dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "W8WmM0-WckHl"
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "    'L_class_valid', 'L_class', \n",
    "    'L_cons', 'L_total',\n",
    "    'Precision', 'Recall', 'F1',\n",
    "    'LR'\n",
    "]\n",
    "stats = pd.DataFrame(columns=['Epoch', *column_names]).set_index('Epoch')\n",
    "best_checkpoint = None\n",
    "\n",
    "for epoch in range(EPOCHS):    \n",
    "    metrics = pd.Series(index=column_names, dtype=float, name=epoch)\n",
    "    \n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(loader_train), total=len(loader_train),\n",
    "                dynamic_ncols=True, leave=False, desc='Training')\n",
    "    for i, (images, attrs) in pbar:\n",
    "        images = images.to(device)\n",
    "        attrs = attrs.to(device).float().unsqueeze(1)\n",
    "\n",
    "        outputs = model(images)\n",
    "        outputs_changed = model(random_change(images))\n",
    "        \n",
    "        loss_classification = bce(outputs, attrs)\n",
    "        loss_consistency = mse(outputs, outputs_changed)\n",
    "        loss_total = loss_classification + CONSISTENCY_WEIGHT * loss_consistency\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss_total.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        metrics.L_class = ema(loss_classification.item(), metrics.L_class)\n",
    "        metrics.L_cons = ema(loss_consistency.item(), metrics.L_cons)\n",
    "        metrics.L_total = ema(loss_total, metrics.L_total)\n",
    "        \n",
    "        lr = scheduler.get_last_lr()[0]\n",
    "        pbar.set_postfix({\n",
    "            'Lclass': f'{loss_classification.item():.3f}',\n",
    "            'Lcons': f'{loss_consistency.item():.3f}',\n",
    "            'LR': f'{lr:.0e}'\n",
    "        }, refresh=False)\n",
    "\n",
    "    loss_valid, precision, recall, f1, confusion_matrix = valid()\n",
    "    scheduler.step()\n",
    "\n",
    "    if best_checkpoint is None or f1 > stats.F1.max():\n",
    "        current_time = datetime.now().isoformat()\n",
    "        best_checkpoint = CHECKPOINTS_PATH / f'classifier-{epoch}-{f1:0.2f}.pth'\n",
    "        torch.save(model.state_dict(), best_checkpoint)\n",
    "    \n",
    "    metrics.L_class_valid = loss_valid\n",
    "    metrics.F1 = f1\n",
    "    metrics.Precision = precision\n",
    "    metrics.Recall = recall\n",
    "    metrics.LR = lr\n",
    "    stats = stats.append(metrics)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    display(HTML(stats.to_html()))\n",
    "\n",
    "model.load_state_dict(torch.load(best_checkpoint))\n",
    "model.eval()\n",
    "\n",
    "print('Best checkpoint:', best_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AZSVWF_rQL47"
   },
   "source": [
    "Now we can draw the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lz43V93gQL47"
   },
   "outputs": [],
   "source": [
    "stats[['L_class', 'L_class_valid']].plot()\n",
    "stats[['F1']].plot()\n",
    "stats[['LR']].plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PrH4iX2gQL49"
   },
   "source": [
    "The learning rates on the train and the validation sets are close so that we can say that the model isn't overfitted.\n",
    "\n",
    "Let's draw a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7UkFmYseQL4_"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.imshow(confusion_matrix, interpolation='nearest', cmap=plt.get_cmap('Blues'))\n",
    "\n",
    "plt.title('Confusion matrix (valid)')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.xticks([0, 1], ['no', 'yes'])\n",
    "plt.yticks([0, 1], ['no', 'yes'])\n",
    "\n",
    "threshold = confusion_matrix.max() / 2\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):\n",
    "        plt.text(j, i, confusion_matrix[i, j],\n",
    "                 horizontalalignment='center',\n",
    "                 color='white' if confusion_matrix[i, j] > threshold else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XHpxVo9dOrnV"
   },
   "source": [
    "We got the model with an F1 score that is sufficient for practical use. Now you can save the weights to your Google Drive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oaepp_en0RmM"
   },
   "source": [
    "# Visualize and analyze the results\n",
    "\n",
    "Let's see which images are the hardest for the model. For that, we have to prepare a dataframe with predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UdWuI7Mkj5e"
   },
   "outputs": [],
   "source": [
    "model.eval()  \n",
    "predictions = []\n",
    "pbar = tqdm(enumerate(loader_valid), total=len(loader_valid))\n",
    "for i, (images, _) in pbar:\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(images)\n",
    "    outputs = outputs.squeeze(1)\n",
    "    predictions.extend(x.item() for x in (outputs.cpu() > 0).int())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoM70OVNP5iU"
   },
   "source": [
    "We got a list of predictions and now we can add it as a column to a dataframe with information about the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RYAm6QySP_Mv"
   },
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(\n",
    "    {'image': df_valid.index, 'y_true': df_valid[MAIN_ATTRIBUTE], 'y_pred': predictions}\n",
    ").set_index('image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tuvIZC7sQk0k"
   },
   "outputs": [],
   "source": [
    "df_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9K6va0TaTlcv"
   },
   "source": [
    "We can filter the dataframe to find false-negative results and show the corresponding images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EvQoI2SdQm-J"
   },
   "outputs": [],
   "source": [
    "df_false_negative = df_results[(df_results.y_true == 1) & (df_results.y_pred == 0)]\n",
    "df_false_negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DrUB4Iy1Q6Ll"
   },
   "outputs": [],
   "source": [
    "load_and_show(df_false_negative.index[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8xnRIl9yTxEL"
   },
   "source": [
    "As you can see, some people don't wear eyeglasses or the eyeglasses are nearly invisible, so the results for them were correct. It's because of the noisy dataset.\n",
    "\n",
    "What about false-positive?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IHCaB9c1Q3Nd"
   },
   "outputs": [],
   "source": [
    "df_false_positive = df_results[(df_results.y_true == 0) & (df_results.y_pred == 1)]\n",
    "df_false_positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SPoXi0lhRUxU"
   },
   "outputs": [],
   "source": [
    "load_and_show(df_false_positive.index[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gCyecjH6Uizs"
   },
   "source": [
    "For these images, the model gave incorrect answers. For some reason, these images are hard cases for the model. However, the overall result is good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Eg2eYxJP0Rw3"
   },
   "source": [
    "# Convert the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H42VbcbSVBgX"
   },
   "source": [
    "It's time to convert our model to a universal format ONNX so that the model could be used in a lens.\n",
    "\n",
    "Remember, we trained our model on images with pixel values from the range `[0, 1]`? By default, Lens Studio passes images with the range `[0, 255]` to the input. There are several options:\n",
    "\n",
    "- Set scale and bias coefficients in Lens Studio on importing the model.\n",
    "- Divide the input image by 255 right inside the model or equivalently divide the weights of the first convolution.\n",
    "- Create a wrapper for the model that prepares inputs (we don't recommend you do that because it adds axtra operations to the model).\n",
    "\n",
    "We will choose the second option, but if you think that the approach is tricky, you can set normalization options in Lens Studio. Anyway, it's very important to use the same normalization both for training and production.\n",
    "\n",
    "You don't need to fuse batchnorms manually. They will be fused into the convolutions by the converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p_Ri1Bxcd3x8",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_weights = torch.load(best_checkpoint)\n",
    "model_weights['blocks.0.weight'] /= 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lDAY0P87QL5R"
   },
   "source": [
    "Also, it would be convenient to add a sigmoid to the model. For that we will create a wrapper that will override `forward()` method of the model. The weights will be compatible with the wrapper because we won't change the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYFKClR6QL5R"
   },
   "outputs": [],
   "source": [
    "class ClassifierExport(Classifier):\n",
    "    def forward(self, x):\n",
    "        y = super().forward(x)\n",
    "        return torch.sigmoid(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6NfdbihWDAP"
   },
   "source": [
    "Now we can create an instance of the wrapper and save the converted model to a file.\n",
    "\n",
    "We use names `input` and `prob` in the model because the same names are used in the lens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvmAxXzhBD6x"
   },
   "outputs": [],
   "source": [
    "model_export = ClassifierExport()\n",
    "model_export.load_state_dict(model_weights)\n",
    "model_export.eval()\n",
    "\n",
    "dummy_input = torch.randn(1, 3, IMAGE_SIZE, IMAGE_SIZE)\n",
    "torch.onnx.export(model_export, dummy_input,\n",
    "                  ONNX_PATH, \n",
    "                  input_names=['input'],\n",
    "                  output_names=['prob']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tb0wUYHhfBJ5"
   },
   "source": [
    "If you use Google Colab you can download the model with the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e54Rgzt4efU5"
   },
   "outputs": [],
   "source": [
    "def download_onnx_from_colab():\n",
    "    from google.colab import files\n",
    "    files.download(ONNX_PATH)\n",
    "\n",
    "## Uncomment for downloading ONNX from Colab\n",
    "# download_onnx_from_colab()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "eyeglasses_classification.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

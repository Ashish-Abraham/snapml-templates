{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image-to-Image translation with SnapML.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXMO-ZPJ4hIu"
      },
      "source": [
        "# Image-to-Image translation with SnapML\n",
        "### What are image-to-image models\n",
        "Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image. It can be applied to a wide range of applications, such as collection style transfer, object transfiguration, season transfer and photo enhancement. One of the best approaches to these tasks is to use Generative Adverseral Networks (GANs).\n",
        "\n",
        "**Note**: It is recommended to upload and run this notebook in Google Colab. You may need to modify some code to suppress warnings/errors running locally as some functions depends on Colab only libraries. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xL49apo7DpCA"
      },
      "source": [
        "### Examples\n",
        "Some popular Image-to-image architectures consistst of:\n",
        "1. Pix2Pix (https://github.com/phillipi/pix2pix)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "Nwg_ey70BBoj",
        "outputId": "f34d379c-64b7-4c38-bdc2-426c4e285442"
      },
      "source": [
        "from IPython.display import Image\n",
        "Image(url='https://github.com/phillipi/pix2pix/raw/master/imgs/examples.jpg', width = 800)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/phillipi/pix2pix/raw/master/imgs/examples.jpg\" width=\"800\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ch1Fa9SyDwyv"
      },
      "source": [
        "2. CycleGAN (https://github.com/junyanz/CycleGAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "id": "vpO_Z17WAY6A",
        "outputId": "7721737f-64fd-457f-cd6a-eee71c710d8d"
      },
      "source": [
        "Image(url='https://github.com/junyanz/CycleGAN/raw/master/imgs/horse2zebra.gif')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/junyanz/CycleGAN/raw/master/imgs/horse2zebra.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuFinNo0D_g8"
      },
      "source": [
        "3. BicycleGAN (https://github.com/junyanz/BicycleGAN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "u_aLavT_D0HU",
        "outputId": "df973f38-739d-4a40-cfbe-0c1804998411"
      },
      "source": [
        "Image(url='https://github.com/junyanz/BicycleGAN/raw/master/imgs/day2night.gif')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"https://github.com/junyanz/BicycleGAN/raw/master/imgs/day2night.gif\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyY9nXSE5W0t"
      },
      "source": [
        "### Pros and Cons of GANs\n",
        "GANs or Conditional GANs have been successful ingenerating hig-fidelity images. However, thye often suffer from:\n",
        "\n",
        "\n",
        "*   Tremendous computational costs\n",
        "*   Bulky memory usage\n",
        "\n",
        "ALl these makes running the original Image-to-Image models almost impossible on mobile devices\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yO2CjdyifR1x"
      },
      "source": [
        "### GAN Compression techiniques and CAT\n",
        "In order to make GANs smaller while preserving the quality, there are many successful GAN compression techniques:\n",
        "\n",
        "*   **GAN Compression** - Efficient Architectures for Interactive Conditional GANs  [[paper](https://arxiv.org/pdf/2003.08936.pdf)]\n",
        "*   **GAN-slimming** - All-in-One GAN Compression by A Unified Optimization Framework  [[paper](https://arxiv.org/pdf/2008.11062.pdf)]\n",
        "\n",
        "And in this notebook, we are going to demonstrate how to use a novel techinque where a teacher model can both provide a search space and perform distillation for student network [[paper](https://arxiv.org/pdf/2103.03467.pdf)] [[code](https://github.com/snap-research/CAT)]\n",
        "\n",
        "With this techniques, we are able to run distilled generative models on most mobile devices at real time while preserving the same image quality.\n",
        "Please see below a sample output for transforming images to  [Ukiyo-e](https://en.wikipedia.org/wiki/Ukiyo-e) paintings\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "id": "TF9MUWIwiwBl",
        "outputId": "d298c95b-484e-48d1-fcb6-eec6a1db7598"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "urls = ['https://raw.githubusercontent.com/snap-research/CAT/tutorial/demo/original.gif',\n",
        "        'https://raw.githubusercontent.com/snap-research/CAT/tutorial/demo/ukiyoe.gif']\n",
        "\n",
        "def make_html(image):\n",
        "     return '<img src=\"{}\" style=\"display:inline;margin:1px\"/>'.format(image)\n",
        "\n",
        "display(HTML(''.join(make_html(url) for url in urls)))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"https://raw.githubusercontent.com/snap-research/CAT/tutorial/demo/original.gif\" style=\"display:inline;margin:1px\"/><img src=\"https://raw.githubusercontent.com/snap-research/CAT/tutorial/demo/ukiyoe.gif\" style=\"display:inline;margin:1px\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yiDIFSRgW2R"
      },
      "source": [
        "### Pre-requisites\n",
        "*   Pretrained teacher (original) model \n",
        "*   Pretrained discriminator model \n",
        "*   Dataset statistics for FID computation\n",
        "\n",
        "We provide the pre-requisites for ukiyoe2photo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_1TM5wXB4xU"
      },
      "source": [
        "### Get training code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TSb3-gjFLFX",
        "outputId": "c770077f-d1a1-417e-c3e9-34b06647c290"
      },
      "source": [
        "# install dependencies\n",
        "%pip install tensorboardX onnx\n",
        "!pip install torch==1.7 torchvision==0.8.0 torchaudio==0.7.0\n",
        "\n",
        "!git clone https://github.com/snap-research/CAT.git\n",
        "%cd CAT\n",
        "!git fetch \n",
        "!git checkout tutorial"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting onnx\n",
            "  Downloading onnx-1.10.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (12.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.7 MB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.2.1 in /usr/local/lib/python3.7/dist-packages (from onnx) (3.10.0.2)\n",
            "Installing collected packages: tensorboardX, onnx\n",
            "Successfully installed onnx-1.10.2 tensorboardX-2.4.1\n",
            "Collecting torch==1.7\n",
            "  Downloading torch-1.7.0-cp37-cp37m-manylinux1_x86_64.whl (776.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.7 MB 4.2 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.0\n",
            "  Downloading torchvision-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (11.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.8 MB 31.9 MB/s \n",
            "\u001b[?25hCollecting torchaudio==0.7.0\n",
            "  Downloading torchaudio-0.7.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 30.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (1.19.5)\n",
            "Collecting dataclasses\n",
            "  Downloading dataclasses-0.6-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.7) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.0) (7.1.2)\n",
            "Installing collected packages: dataclasses, torch, torchvision, torchaudio\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "  Attempting uninstall: torchaudio\n",
            "    Found existing installation: torchaudio 0.10.0+cu111\n",
            "    Uninstalling torchaudio-0.10.0+cu111:\n",
            "      Successfully uninstalled torchaudio-0.10.0+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.0 which is incompatible.\u001b[0m\n",
            "Successfully installed dataclasses-0.6 torch-1.7.0 torchaudio-0.7.0 torchvision-0.8.0\n",
            "Cloning into 'CAT'...\n",
            "remote: Enumerating objects: 229, done.\u001b[K\n",
            "remote: Counting objects: 100% (229/229), done.\u001b[K\n",
            "remote: Compressing objects: 100% (152/152), done.\u001b[K\n",
            "remote: Total 229 (delta 104), reused 187 (delta 71), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (229/229), 169.35 KiB | 4.98 MiB/s, done.\n",
            "Resolving deltas: 100% (104/104), done.\n",
            "/content/CAT\n",
            "Branch 'tutorial' set up to track remote branch 'tutorial' from 'origin'.\n",
            "Switched to a new branch 'tutorial'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmd_i8G9eTUU"
      },
      "source": [
        "### Download dataset\n",
        "Download one of the public ukiyoe2photo datasets.\n",
        "\n",
        "\n",
        "Or use your own dataset by creating the appropriate folders and adding in the images. Follow the instructions [here](https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix/blob/master/docs/datasets.md#pix2pix-datasets)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QHYf1vAVCYDj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cedbbf50-30be-4f79-b797-a0b837808b56"
      },
      "source": [
        "# example for ukiyoe2photo dataset on CycleGAN\n",
        "# properties: Used on CycleGAN \n",
        "#             Unpaired dataset \n",
        "import os\n",
        "\n",
        "FILE = 'ukiyoe2photo' \n",
        "\n",
        "BASE_DATA_DIR = './database/'\n",
        "FILE_ZIP = FILE+'.zip'\n",
        "URL = 'https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/'+FILE_ZIP \n",
        "TARGET_DIR = os.path.join(BASE_DATA_DIR, FILE)\n",
        "\n",
        "!mkdir -p $BASE_DATA_DIR\n",
        "!wget -N $URL -O $FILE_ZIP\n",
        "!mkdir -p $TARGET_DIR\n",
        "!unzip -q $FILE_ZIP -d $BASE_DATA_DIR \n",
        "!rm $FILE_ZIP"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: timestamping does nothing in combination with -O. See the manual\n",
            "for details.\n",
            "\n",
            "--2022-01-12 03:18:20--  https://people.eecs.berkeley.edu/~taesung_park/CycleGAN/datasets/ukiyoe2photo.zip\n",
            "Resolving people.eecs.berkeley.edu (people.eecs.berkeley.edu)... 128.32.244.190\n",
            "Connecting to people.eecs.berkeley.edu (people.eecs.berkeley.edu)|128.32.244.190|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 292946532 (279M) [application/zip]\n",
            "Saving to: ‘ukiyoe2photo.zip’\n",
            "\n",
            "ukiyoe2photo.zip    100%[===================>] 279.38M  2.38MB/s    in 3m 24s  \n",
            "\n",
            "2022-01-12 03:21:44 (1.37 MB/s) - ‘ukiyoe2photo.zip’ saved [292946532/292946532]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XE7n45NHez9g"
      },
      "source": [
        "# if valA , valB don't exist \n",
        "# copy or create a softlink as you wish \n",
        "\n",
        "!cp -r $TARGET_DIR/testA/  $TARGET_DIR/valA\n",
        "!cp -r $TARGET_DIR/testB/ $TARGET_DIR/valB"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dz3idc2dYQGa"
      },
      "source": [
        "### Download pre-trained teacher models \n",
        "\n",
        "Download our pre-trained teacher models (generator and discriminator) and the calculated statistics used for FID.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7-gldeZ6mYLv",
        "outputId": "55eca84e-e523-4c79-9512-0f364f6aec63"
      },
      "source": [
        "# download ukyio teacher generator\n",
        "!gdown --id 1_VrjIowjfj-CxA5LX-P5vMcDY8pBPwu_\n",
        "# download ukyio teacher discriminator\n",
        "!gdown --id 1dM_qHcyx_XTBRTsSg82HMgUAHw8zElA4\n",
        "# download ukyio real stats\n",
        "!gdown --id 1LfFto-IhmpjRktkGlAETfiikVUpabxmv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1_VrjIowjfj-CxA5LX-P5vMcDY8pBPwu_\n",
            "To: /content/CAT/ukiyo_teacher_iter68000_net_G_B.pth\n",
            "100% 32.8M/32.8M [00:00<00:00, 124MB/s] \n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1dM_qHcyx_XTBRTsSg82HMgUAHw8zElA4\n",
            "To: /content/CAT/ukiyo_teacher_iter68000_net_D_B.pth\n",
            "100% 22.2M/22.2M [00:00<00:00, 135MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LfFto-IhmpjRktkGlAETfiikVUpabxmv\n",
            "To: /content/CAT/ukiyo_A.npz\n",
            "100% 33.6M/33.6M [00:00<00:00, 127MB/s] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeUPEdQ-a0s7"
      },
      "source": [
        "## Training NPU-Friendly student model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4F3hhW1flaA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e39222bd-e94b-4c3d-f2cd-ec847c177a4b"
      },
      "source": [
        "# The teacher model is trained with instance normalization and reflect padding\n",
        "# For NPU-Friendly student model, we prune the teacher model to the target FLOPs, \n",
        "# and replace the padding to zero padding, normalization to batch norm w/ \n",
        "# tracking running stats.\n",
        "\n",
        "NUM_EPOCHS = 50 \n",
        "\n",
        "!python distill.py --dataroot ./database/ukiyoe2photo \\\n",
        "--log_dir logs/cycle_gan/ukiyoe2photo/inception/student/2p6B \\\n",
        "--restore_teacher_G_path ukiyo_teacher_iter68000_net_G_B.pth \\\n",
        "--restore_pretrained_G_path ukiyo_teacher_iter68000_net_G_B.pth \\\n",
        "--restore_D_path ukiyo_teacher_iter68000_net_D_B.pth \\\n",
        "--real_stat_path ukiyo_A.npz \\\n",
        "--dataset_mode unaligned \\\n",
        "--distiller inception \\\n",
        "--gan_mode lsgan \\\n",
        "--nepochs $NUM_EPOCHS --nepochs_decay $NUM_EPOCHS \\\n",
        "--teacher_netG inception_9blocks --student_netG inception_9blocks \\\n",
        "--pretrained_ngf 64 --teacher_ngf 64 --student_ngf 20 \\\n",
        "--ndf 64 \\\n",
        "--num_threads 2 \\\n",
        "--eval_batch_size 2 \\\n",
        "--batch_size 10 \\\n",
        "--gpu_ids 0 \\\n",
        "--norm_affine \\\n",
        "--norm_affine_D \\\n",
        "--channels_reduction_factor 6 \\\n",
        "--kernel_sizes 1 3 5 \\\n",
        "--lambda_distill 2.8 \\\n",
        "--lambda_recon 1000 \\\n",
        "--prune_cin_lb 16 \\\n",
        "--target_flops 2.6e9 \\\n",
        "--distill_G_loss_type ka \\\n",
        "--netD multi_scale \\\n",
        "--crop_size 512,256 \\\n",
        "--preprocess resize_and_crop \\\n",
        "--load_size 600 \\\n",
        "--save_epoch_freq 1 \\\n",
        "--save_latest_freq 500 \\\n",
        "--direction BtoA \\\n",
        "--norm_student batch \\\n",
        "--padding_type_student zero \\\n",
        "--norm_affine_student \\\n",
        "--norm_track_running_stats_student"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------- Options ---------------\n",
            "                active_fn: nn.ReLU                       \n",
            "              active_fn_D: nn.LeakyReLU                  \n",
            "             aspect_ratio: 1.0                           \n",
            "               batch_size: 10                            \t[default: 1]\n",
            "                    beta1: 0.5                           \n",
            "                 channels: None                          \n",
            "channels_reduction_factor: 6                             \t[default: 1]\n",
            "          cityscapes_path: database/cityscapes-origin    \n",
            "                crop_size: 512,256                       \t[default: 256, 256]\n",
            "                 dataroot: ./database/ukiyoe2photo       \t[default: None]\n",
            "             dataset_mode: unaligned                     \t[default: aligned]\n",
            "                direction: BtoA                          \t[default: AtoB]\n",
            "          display_winsize: 256                           \n",
            "      distill_G_loss_type: ka                            \t[default: mse]\n",
            "                distiller: inception                     \n",
            "                 drn_path: drn-d-105_ms_cityscapes.pth   \n",
            "             dropout_rate: 0                             \n",
            "               epoch_base: 1                             \n",
            "          eval_batch_size: 2                             \t[default: 1]\n",
            "                 gan_mode: lsgan                         \t[default: hinge]\n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                iter_base: 1                             \n",
            "             kernel_sizes: [1, 3, 5]                     \t[default: [3, 5, 7]]\n",
            "           lambda_distill: 2.8                           \t[default: 1]\n",
            "               lambda_gan: 1                             \n",
            "             lambda_recon: 1000.0                        \t[default: 100]\n",
            "           load_in_memory: False                         \n",
            "                load_size: 600                           \t[default: 286]\n",
            "                  log_dir: logs/cycle_gan/ukiyoe2photo/inception/student/2p6B\t[default: logs/inception]\n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: -1                            \n",
            "     moving_average_decay: 0.0                           \n",
            "moving_average_decay_adjust: False                         \n",
            "moving_average_decay_base_batch: 32                            \n",
            "               n_layers_D: 4                             \n",
            "                      ndf: 64                            \t[default: 128]\n",
            "                  nepochs: 1                             \t[default: 5]\n",
            "            nepochs_decay: 1                             \t[default: 15]\n",
            "                     netD: multi_scale                   \t[default: n_layers]\n",
            "                  no_flip: False                         \n",
            "                     norm: instance                      \n",
            "                   norm_D: spectralinstance              \n",
            "              norm_affine: True                          \t[default: False]\n",
            "            norm_affine_D: True                          \t[default: False]\n",
            "      norm_affine_student: True                          \t[default: False]\n",
            "             norm_epsilon: 1e-05                         \n",
            "            norm_momentum: 0.1                           \n",
            "             norm_student: batch                         \t[default: instance]\n",
            " norm_track_running_stats: False                         \n",
            "norm_track_running_stats_student: True                          \t[default: False]\n",
            "                    num_D: 2                             \n",
            "              num_threads: 2                             \t[default: 4]\n",
            "                output_nc: 3                             \n",
            "             padding_type: reflect                       \n",
            "     padding_type_student: zero                          \n",
            "                    phase: train                         \n",
            "               preprocess: resize_and_crop               \n",
            "          pretrained_netG: inception_9blocks             \n",
            "           pretrained_ngf: 64                            \n",
            "pretrained_student_G_path: None                          \n",
            "               print_freq: 100                           \n",
            "             prune_cin_lb: 16                            \t[default: 0]\n",
            "           prune_continue: False                         \n",
            "    prune_logging_verbose: False                         \n",
            "               prune_only: False                         \n",
            "           real_stat_path: ukiyo_A.npz                   \t[default: None]\n",
            "          recon_loss_type: l1                            \n",
            "           restore_A_path: None                          \n",
            "           restore_D_path: ukiyo_teacher_iter68000_net_D_B.pth\t[default: None]\n",
            "           restore_O_path: None                          \n",
            "restore_pretrained_G_path: ukiyo_teacher_iter68000_net_G_B.pth\t[default: None]\n",
            "   restore_student_G_path: None                          \n",
            "   restore_teacher_G_path: ukiyo_teacher_iter68000_net_G_B.pth\t[default: None]\n",
            "          save_epoch_freq: 1                             \t[default: 5]\n",
            "         save_latest_freq: 500                           \t[default: 20000]\n",
            "                     seed: 233                           \n",
            "           serial_batches: False                         \n",
            "     student_dropout_rate: 0                             \n",
            "             student_netG: inception_9blocks             \n",
            "              student_ngf: 20                            \t[default: 48]\n",
            "               table_path: datasets/table.txt            \n",
            "             target_flops: 2600000000.0                  \t[default: 0]\n",
            "     teacher_dropout_rate: 0                             \n",
            "             teacher_netG: inception_9blocks             \n",
            "              teacher_ngf: 64                            \n",
            "          tensorboard_dir: None                          \n",
            "----------------- End -------------------\n",
            "distill.py --dataroot ./database/ukiyoe2photo --log_dir logs/cycle_gan/ukiyoe2photo/inception/student/2p6B --restore_teacher_G_path ukiyo_teacher_iter68000_net_G_B.pth --restore_pretrained_G_path ukiyo_teacher_iter68000_net_G_B.pth --restore_D_path ukiyo_teacher_iter68000_net_D_B.pth --real_stat_path ukiyo_A.npz --dataset_mode unaligned --distiller inception --gan_mode lsgan --nepochs 1 --nepochs_decay 1 --teacher_netG inception_9blocks --student_netG inception_9blocks --pretrained_ngf 64 --teacher_ngf 64 --student_ngf 20 --ndf 64 --num_threads 2 --eval_batch_size 2 --batch_size 10 --gpu_ids 0 --norm_affine --norm_affine_D --channels_reduction_factor 6 --kernel_sizes 1 3 5 --lambda_distill 2.8 --lambda_recon 1000 --prune_cin_lb 16 --target_flops 2.6e9 --distill_G_loss_type ka --netD multi_scale --crop_size 512,256 --preprocess resize_and_crop --load_size 600 --save_epoch_freq 1 --save_latest_freq 500 --direction BtoA --norm_student batch --padding_type_student zero --norm_affine_student --norm_track_running_stats_student\n",
            "dataset [UnalignedDataset] was created\n",
            "The number of training images = 6286\n",
            "data shape is: channel=3, height=512, width=256.\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "dataset [SingleDataset] was created\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://github.com/mseitzer/pytorch-fid/releases/download/fid_weights/pt_inception-2015-12-05-6726825d.pth\" to /root/.cache/torch/hub/checkpoints/pt_inception-2015-12-05-6726825d.pth\n",
            "100% 91.2M/91.2M [00:00<00:00, 97.2MB/s]\n",
            "distiller [InceptionDistiller] was created\n",
            "Load network at ukiyo_teacher_iter68000_net_G_B.pth\n",
            "Load network at ukiyo_teacher_iter68000_net_D_B.pth\n",
            "---------- Networks initialized -------------\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 20, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
            "    (2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (5): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(80, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(40, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(20, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_student] Total number of parameters : 0.797 M\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_teacher] Total number of parameters : 8.154 M\n",
            "MultiscaleDiscriminator(\n",
            "  (discriminator_0): SPADENLayerDiscriminator(\n",
            "    (model0): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model4): Sequential(\n",
            "      (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (discriminator_1): SPADENLayerDiscriminator(\n",
            "    (model0): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model4): Sequential(\n",
            "      (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[Network netD] Total number of parameters : 5.528 M\n",
            "-----------------------------------------------\n",
            "features.0.res_ops.0.1.1.weight\n",
            "features.0.res_ops.1.1.1.weight\n",
            "features.0.res_ops.2.1.1.weight\n",
            "features.0.dw_ops.0.0.1.weight\n",
            "features.0.dw_ops.1.0.1.weight\n",
            "features.0.dw_ops.2.0.1.weight\n",
            "features.1.res_ops.0.1.1.weight\n",
            "features.1.res_ops.1.1.1.weight\n",
            "features.1.res_ops.2.1.1.weight\n",
            "features.1.dw_ops.0.0.1.weight\n",
            "features.1.dw_ops.1.0.1.weight\n",
            "features.1.dw_ops.2.0.1.weight\n",
            "features.2.res_ops.0.1.1.weight\n",
            "features.2.res_ops.1.1.1.weight\n",
            "features.2.res_ops.2.1.1.weight\n",
            "features.2.dw_ops.0.0.1.weight\n",
            "features.2.dw_ops.1.0.1.weight\n",
            "features.2.dw_ops.2.0.1.weight\n",
            "features.3.res_ops.0.1.1.weight\n",
            "features.3.res_ops.1.1.1.weight\n",
            "features.3.res_ops.2.1.1.weight\n",
            "features.3.dw_ops.0.0.1.weight\n",
            "features.3.dw_ops.1.0.1.weight\n",
            "features.3.dw_ops.2.0.1.weight\n",
            "features.4.res_ops.0.1.1.weight\n",
            "features.4.res_ops.1.1.1.weight\n",
            "features.4.res_ops.2.1.1.weight\n",
            "features.4.dw_ops.0.0.1.weight\n",
            "features.4.dw_ops.1.0.1.weight\n",
            "features.4.dw_ops.2.0.1.weight\n",
            "features.5.res_ops.0.1.1.weight\n",
            "features.5.res_ops.1.1.1.weight\n",
            "features.5.res_ops.2.1.1.weight\n",
            "features.5.dw_ops.0.0.1.weight\n",
            "features.5.dw_ops.1.0.1.weight\n",
            "features.5.dw_ops.2.0.1.weight\n",
            "features.6.res_ops.0.1.1.weight\n",
            "features.6.res_ops.1.1.1.weight\n",
            "features.6.res_ops.2.1.1.weight\n",
            "features.6.dw_ops.0.0.1.weight\n",
            "features.6.dw_ops.1.0.1.weight\n",
            "features.6.dw_ops.2.0.1.weight\n",
            "features.7.res_ops.0.1.1.weight\n",
            "features.7.res_ops.1.1.1.weight\n",
            "features.7.res_ops.2.1.1.weight\n",
            "features.7.dw_ops.0.0.1.weight\n",
            "features.7.dw_ops.1.0.1.weight\n",
            "features.7.dw_ops.2.0.1.weight\n",
            "features.8.res_ops.0.1.1.weight\n",
            "features.8.res_ops.1.1.1.weight\n",
            "features.8.res_ops.2.1.1.weight\n",
            "features.8.dw_ops.0.0.1.weight\n",
            "features.8.dw_ops.1.0.1.weight\n",
            "features.8.dw_ops.2.0.1.weight\n",
            "scale range: [0.30794793367385864, 1.434011697769165]\n",
            "scale threshold: 1.127203345298767, searched flops: 2594521088, target flops: 2600000000.0, flops diff: -5478912.0.\n",
            "All layers are pruned.\n",
            "initialize network with normal\n",
            "---------- Networks initialized -------------\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ConstantPad2d(padding=(3, 3, 3, 3), value=0.0)\n",
            "    (1): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(16, 22, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(22, 138, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): BatchNorm2d(138, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(138, 138, res_channels=[37, 0, 0], dw_channels=[6, 7, 0], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(138, 138, res_channels=[29, 0, 0], dw_channels=[2, 5, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(138, 138, res_channels=[34, 0, 0], dw_channels=[4, 5, 0], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(138, 138, res_channels=[26, 0, 0], dw_channels=[4, 4, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(138, 138, res_channels=[23, 0, 0], dw_channels=[3, 3, 2], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(138, 138, res_channels=[24, 0, 0], dw_channels=[5, 1, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(138, 138, res_channels=[27, 0, 0], dw_channels=[2, 1, 0], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(138, 138, res_channels=[19, 0, 0], dw_channels=[1, 0, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(138, 138, res_channels=[15, 0, 0], dw_channels=[3, 1, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(138, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConstantPad2d(padding=(3, 3, 3, 3), value=0.0)\n",
            "    (7): Conv2d(16, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_student] Total number of parameters : 0.149 M\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_teacher] Total number of parameters : 8.154 M\n",
            "MultiscaleDiscriminator(\n",
            "  (discriminator_0): SPADENLayerDiscriminator(\n",
            "    (model0): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model4): Sequential(\n",
            "      (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "  )\n",
            "  (discriminator_1): SPADENLayerDiscriminator(\n",
            "    (model0): Sequential(\n",
            "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2))\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model1): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model2): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model3): Sequential(\n",
            "      (0): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "        (1): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "      )\n",
            "      (1): LeakyReLU(negative_slope=0.2)\n",
            "    )\n",
            "    (model4): Sequential(\n",
            "      (0): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(2, 2))\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[Network netD] Total number of parameters : 5.528 M\n",
            "-----------------------------------------------\n",
            "netG student FLOPs: 2574811136; down sampling: 635928576; features: 677445632; up sampling: 1261436928.\n",
            "100% 375/375 [03:01<00:00,  2.06it/s]\n",
            "tcmalloc: large alloc 1179648000 bytes == 0x5580a2028000 @  0x7fd3afc01b6b 0x7fd3afc21379 0x7fd34eca474e 0x7fd34eca67b6 0x7fd38d90bd53 0x7fd38d28654a 0x7fd38d5e0c0a 0x7fd38d608803 0x7fd38d78eb14 0x7fd38d8cb4ee 0x7fd38d324976 0x7fd38d325b30 0x7fd38d5e2b09 0x7fd38ce61249 0x7fd38d77bae8 0x7fd38d6878a5 0x7fd38d32741b 0x7fd38d8177d8 0x7fd38ce61249 0x7fd38d77bae8 0x7fd38d6879f5 0x7fd38ec5b997 0x7fd38ce61249 0x7fd38d77bae8 0x7fd38d6879f5 0x7fd39d4a178e 0x558013379544 0x558013379240 0x5580133ed627 0x5580133e79ee 0x55801337abda\n",
            "tcmalloc: large alloc 2359296000 bytes == 0x55810a548000 @  0x7fd3afc1f1e7 0x7fd3ad0cc46e 0x7fd3ad11cc7b 0x7fd3ad11cebe 0x7fd3ad1b5887 0x5580133794b0 0x55801346ae1d 0x5580133ece99 0x5580133e79ee 0x55801337abda 0x5580133e9737 0x5580133e79ee 0x55801337abda 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x5580133e79ee 0x5580133e76f3 0x5580134b14c2 0x5580134b183d 0x5580134b16e6 0x558013489163 0x558013488e0c 0x7fd3aea09bf7 0x558013488cea\n",
            "100% 375/375 [00:21<00:00, 17.69it/s]\n",
            "###(Evaluate epoch: 1, iters: 1, time: 235.990) fid: 434.691 fid-mean: 434.691 fid-best: 434.691 \n",
            "Saving the latest model (epoch 1, total_steps 1)\n",
            "(epoch: 1, iters: 100, time: 3.664) G_gan: 2.173 G_distill: -9.629 G_recon: 190.100 D_fake: 0.057 D_real: 0.054 \n",
            "(epoch: 1, iters: 200, time: 3.714) G_gan: 2.342 G_distill: -10.095 G_recon: 204.689 D_fake: 0.110 D_real: 0.041 \n",
            "(epoch: 1, iters: 300, time: 3.715) G_gan: 2.024 G_distill: -10.339 G_recon: 145.377 D_fake: 0.035 D_real: 0.021 \n",
            "(epoch: 1, iters: 400, time: 3.732) G_gan: 1.841 G_distill: -9.996 G_recon: 175.873 D_fake: 0.025 D_real: 0.036 \n",
            "(epoch: 1, iters: 500, time: 3.722) G_gan: 1.746 G_distill: -10.794 G_recon: 154.951 D_fake: 0.038 D_real: 0.078 \n",
            "100% 375/375 [03:01<00:00,  2.06it/s]\n",
            "tcmalloc: large alloc 2359296000 bytes == 0x55810a3ac000 @  0x7fd3afc1f1e7 0x7fd3ad0cc46e 0x7fd3ad11cc7b 0x7fd3ad11cebe 0x7fd3ad1b5887 0x5580133794b0 0x55801346ae1d 0x5580133ece99 0x5580133e79ee 0x55801337abda 0x5580133e9737 0x5580133e79ee 0x55801337abda 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x5580133e79ee 0x5580133e76f3 0x5580134b14c2 0x5580134b183d 0x5580134b16e6 0x558013489163 0x558013488e0c 0x7fd3aea09bf7 0x558013488cea\n",
            "100% 375/375 [00:22<00:00, 16.73it/s]\n",
            "###(Evaluate epoch: 1, iters: 500, time: 256.204) fid: 256.952 fid-mean: 345.821 fid-best: 256.952 \n",
            "Saving the latest model (epoch 1, total_steps 500)\n",
            "(epoch: 1, iters: 600, time: 3.716) G_gan: 1.962 G_distill: -10.392 G_recon: 149.805 D_fake: 0.020 D_real: 0.041 \n",
            "End of epoch 1 / 2 \t Time Taken: 2851.56 sec\n",
            "100% 375/375 [03:01<00:00,  2.07it/s]\n",
            "tcmalloc: large alloc 2359296000 bytes == 0x55810a46c000 @  0x7fd3afc1f1e7 0x7fd3ad0cc46e 0x7fd3ad11cc7b 0x7fd3ad11cebe 0x7fd3ad1b5887 0x5580133794b0 0x55801346ae1d 0x5580133ece99 0x5580133e79ee 0x55801337abda 0x5580133e9737 0x5580133e79ee 0x55801337abda 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x5580133e79ee 0x5580133e76f3 0x5580134b14c2 0x5580134b183d 0x5580134b16e6 0x558013489163 0x558013488e0c 0x7fd3aea09bf7 0x558013488cea\n",
            "100% 375/375 [00:23<00:00, 16.23it/s]\n",
            "###(Evaluate epoch: 1, iters: 630, time: 256.729) fid: 259.717 fid-mean: 317.120 fid-best: 256.952 \n",
            "Saving the model at the end of epoch 1, iters 630\n",
            "learning rate = 0.0001000\n",
            "\n",
            "(epoch: 2, iters: 700, time: 3.710) G_gan: 1.877 G_distill: -10.338 G_recon: 175.539 D_fake: 0.030 D_real: 0.038 \n",
            "(epoch: 2, iters: 800, time: 3.729) G_gan: 1.930 G_distill: -10.225 G_recon: 145.835 D_fake: 0.018 D_real: 0.012 \n",
            "(epoch: 2, iters: 900, time: 3.706) G_gan: 2.009 G_distill: -10.624 G_recon: 188.754 D_fake: 0.021 D_real: 0.023 \n",
            "(epoch: 2, iters: 1000, time: 3.696) G_gan: 2.094 G_distill: -10.495 G_recon: 136.595 D_fake: 0.020 D_real: 0.021 \n",
            "100% 375/375 [02:59<00:00,  2.09it/s]\n",
            "tcmalloc: large alloc 2359296000 bytes == 0x55810a3ac000 @  0x7fd3afc1f1e7 0x7fd3ad0cc46e 0x7fd3ad11cc7b 0x7fd3ad11cebe 0x7fd3ad1b5887 0x5580133794b0 0x55801346ae1d 0x5580133ece99 0x5580133e79ee 0x55801337abda 0x5580133e9737 0x5580133e79ee 0x55801337abda 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x5580133e79ee 0x5580133e76f3 0x5580134b14c2 0x5580134b183d 0x5580134b16e6 0x558013489163 0x558013488e0c 0x7fd3aea09bf7 0x558013488cea\n",
            "100% 375/375 [00:22<00:00, 16.61it/s]\n",
            "###(Evaluate epoch: 2, iters: 1000, time: 259.791) fid: 257.120 fid-mean: 257.930 fid-best: 256.952 \n",
            "Saving the latest model (epoch 2, total_steps 1000)\n",
            "(epoch: 2, iters: 1100, time: 3.690) G_gan: 2.030 G_distill: -10.449 G_recon: 195.332 D_fake: 0.022 D_real: 0.014 \n",
            "(epoch: 2, iters: 1200, time: 3.682) G_gan: 1.732 G_distill: -10.241 G_recon: 184.029 D_fake: 0.044 D_real: 0.048 \n",
            "End of epoch 2 / 2 \t Time Taken: 2593.72 sec\n",
            "100% 375/375 [02:59<00:00,  2.09it/s]\n",
            "tcmalloc: large alloc 2359296000 bytes == 0x55810a40c000 @  0x7fd3afc1f1e7 0x7fd3ad0cc46e 0x7fd3ad11cc7b 0x7fd3ad11cebe 0x7fd3ad1b5887 0x5580133794b0 0x55801346ae1d 0x5580133ece99 0x5580133e79ee 0x55801337abda 0x5580133e9737 0x5580133e79ee 0x55801337abda 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x55801337aafa 0x5580133e8c0d 0x5580133e79ee 0x5580133e76f3 0x5580134b14c2 0x5580134b183d 0x5580134b16e6 0x558013489163 0x558013488e0c 0x7fd3aea09bf7 0x558013488cea\n",
            "100% 375/375 [00:22<00:00, 16.69it/s]\n",
            "###(Evaluate epoch: 2, iters: 1259, time: 259.481) fid: 252.111 fid-mean: 256.316 fid-best: 252.111 \n",
            "Saving the model at the end of epoch 2, iters 1259\n",
            "learning rate = 0.0000000\n",
            "\n",
            "Distillation finished!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PafswnCva9NP"
      },
      "source": [
        "### Exporting model to ONNX"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykOlMLkSwErp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d47c6cc-b115-456e-e5f5-e07562ff46cf"
      },
      "source": [
        "%pip install torchprofile\n",
        "\n",
        "!python onnx_export.py --dataroot ./database/ukiyoe2photo \\\n",
        "--log_dir onnx_files/cycle_gan/ukiyoe2photo/inception/student/2p6B \\\n",
        "--restore_teacher_G_path ukiyo_teacher_iter68000_net_G_B.pth \\\n",
        "--pretrained_student_G_path logs/cycle_gan/ukiyoe2photo/inception/student/2p6B/checkpoints/2_net_G.pth \\\n",
        "--real_stat_path ukiyo_A.npz \\\n",
        "--dataset_mode unaligned \\\n",
        "--pretrained_ngf 64 --teacher_ngf 64 --student_ngf 20 \\\n",
        "--gpu_ids 0 \\\n",
        "--norm_affine \\\n",
        "--channels_reduction_factor 6 \\\n",
        "--kernel_sizes 1 3 5 \\\n",
        "--prune_cin_lb 16 \\\n",
        "--target_flops 2.6e9 \\\n",
        "--load_size 600 \\\n",
        "--crop_size 512,256 \\\n",
        "--norm_student batch \\\n",
        "--padding_type_student zero \\\n",
        "--norm_affine_student \\\n",
        "--norm_track_running_stats_student \\"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchprofile\n",
            "  Downloading torchprofile-0.0.4-py3-none-any.whl (7.7 kB)\n",
            "Requirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from torchprofile) (1.19.5)\n",
            "Requirement already satisfied: torchvision>=0.4 in /usr/local/lib/python3.7/dist-packages (from torchprofile) (0.8.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from torchprofile) (1.7.0)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->torchprofile) (0.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->torchprofile) (3.10.0.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->torchprofile) (0.16.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4->torchprofile) (7.1.2)\n",
            "Installing collected packages: torchprofile\n",
            "Successfully installed torchprofile-0.0.4\n",
            "----------------- Options ---------------\n",
            "                active_fn: nn.ReLU                       \n",
            "              active_fn_D: nn.LeakyReLU                  \n",
            "             aspect_ratio: 1.0                           \n",
            "               batch_size: 1                             \n",
            "                    beta1: 0.5                           \n",
            "                 channels: None                          \n",
            "channels_reduction_factor: 6                             \t[default: 1]\n",
            "          cityscapes_path: database/cityscapes-origin    \n",
            "                crop_size: 512,256                       \t[default: 256, 256]\n",
            "                 dataroot: ./database/ukiyoe2photo       \t[default: None]\n",
            "             dataset_mode: unaligned                     \t[default: aligned]\n",
            "                direction: AtoB                          \n",
            "          display_winsize: 256                           \n",
            "      distill_G_loss_type: mse                           \n",
            "                distiller: inception                     \n",
            "                 drn_path: drn-d-105_ms_cityscapes.pth   \n",
            "             dropout_rate: 0                             \n",
            "               epoch_base: 1                             \n",
            "          eval_batch_size: 1                             \n",
            "                 gan_mode: hinge                         \n",
            "                  gpu_ids: 0                             \n",
            "                init_gain: 0.02                          \n",
            "                init_type: normal                        \n",
            "                 input_nc: 3                             \n",
            "                  isTrain: True                          \t[default: None]\n",
            "                iter_base: 1                             \n",
            "             kernel_sizes: [1, 3, 5]                     \t[default: [3, 5, 7]]\n",
            "           lambda_distill: 1                             \n",
            "               lambda_gan: 1                             \n",
            "             lambda_recon: 100                           \n",
            "           load_in_memory: False                         \n",
            "                load_size: 600                           \t[default: 286]\n",
            "                  log_dir: onnx_files/cycle_gan/ukiyoe2photo/inception/student/2p6B\t[default: logs/inception]\n",
            "                       lr: 0.0002                        \n",
            "           lr_decay_iters: 50                            \n",
            "                lr_policy: linear                        \n",
            "         max_dataset_size: -1                            \n",
            "     moving_average_decay: 0.0                           \n",
            "moving_average_decay_adjust: False                         \n",
            "moving_average_decay_base_batch: 32                            \n",
            "               n_layers_D: 3                             \n",
            "                      ndf: 128                           \n",
            "                  nepochs: 5                             \n",
            "            nepochs_decay: 15                            \n",
            "                     netD: n_layers                      \n",
            "                  no_flip: False                         \n",
            "                     norm: instance                      \n",
            "              norm_affine: True                          \t[default: False]\n",
            "            norm_affine_D: False                         \n",
            "      norm_affine_student: True                          \t[default: False]\n",
            "             norm_epsilon: 1e-05                         \n",
            "            norm_momentum: 0.1                           \n",
            "             norm_student: batch                         \t[default: instance]\n",
            " norm_track_running_stats: False                         \n",
            "norm_track_running_stats_student: True                          \t[default: False]\n",
            "              num_threads: 4                             \n",
            "                output_nc: 3                             \n",
            "             padding_type: reflect                       \n",
            "     padding_type_student: zero                          \n",
            "                    phase: train                         \n",
            "               preprocess: resize_and_crop               \n",
            "          pretrained_netG: inception_9blocks             \n",
            "           pretrained_ngf: 64                            \n",
            "pretrained_student_G_path: logs/cycle_gan/ukiyoe2photo/inception/student/2p6B/checkpoints/2_net_G.pth\t[default: None]\n",
            "               print_freq: 100                           \n",
            "             prune_cin_lb: 16                            \t[default: 0]\n",
            "           prune_continue: False                         \n",
            "    prune_logging_verbose: False                         \n",
            "               prune_only: False                         \n",
            "           real_stat_path: ukiyo_A.npz                   \t[default: None]\n",
            "          recon_loss_type: l1                            \n",
            "           restore_A_path: None                          \n",
            "           restore_D_path: None                          \n",
            "           restore_O_path: None                          \n",
            "restore_pretrained_G_path: None                          \n",
            "   restore_student_G_path: None                          \n",
            "   restore_teacher_G_path: ukiyo_teacher_iter68000_net_G_B.pth\t[default: None]\n",
            "          save_epoch_freq: 5                             \n",
            "         save_latest_freq: 20000                         \n",
            "                     seed: 233                           \n",
            "           serial_batches: False                         \n",
            "     student_dropout_rate: 0                             \n",
            "             student_netG: inception_9blocks             \n",
            "              student_ngf: 20                            \t[default: 48]\n",
            "               table_path: datasets/table.txt            \n",
            "             target_flops: 2600000000.0                  \t[default: 0]\n",
            "     teacher_dropout_rate: 0                             \n",
            "             teacher_netG: inception_9blocks             \n",
            "              teacher_ngf: 64                            \n",
            "          tensorboard_dir: None                          \n",
            "----------------- End -------------------\n",
            "onnx_export.py --dataroot ./database/ukiyoe2photo --log_dir onnx_files/cycle_gan/ukiyoe2photo/inception/student/2p6B --restore_teacher_G_path ukiyo_teacher_iter68000_net_G_B.pth --pretrained_student_G_path logs/cycle_gan/ukiyoe2photo/inception/student/2p6B/checkpoints/2_net_G.pth --real_stat_path ukiyo_A.npz --dataset_mode unaligned --pretrained_ngf 64 --teacher_ngf 64 --student_ngf 20 --gpu_ids 0 --norm_affine --channels_reduction_factor 6 --kernel_sizes 1 3 5 --prune_cin_lb 16 --target_flops 2.6e9 --load_size 600 --crop_size 512,256 --norm_student batch --padding_type_student zero --norm_affine_student --norm_track_running_stats_student\n",
            "dataset [UnalignedDataset] was created\n",
            "data shape is: channel=3, height=512, width=256.\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "dataset [SingleDataset] was created\n",
            "/usr/local/lib/python3.7/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "distiller [InceptionDistiller] was created\n",
            "Load network at ukiyo_teacher_iter68000_net_G_B.pth\n",
            "---------- Networks initialized -------------\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 20, kernel_size=(7, 7), stride=(1, 1), bias=False)\n",
            "    (2): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(20, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (5): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(40, 80, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (8): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(80, 80, res_channels=[13, 13, 13], dw_channels=[13, 13, 13], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(80, 40, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(40, 20, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(20, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_student] Total number of parameters : 0.797 M\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_teacher] Total number of parameters : 8.154 M\n",
            "NLayerDiscriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (3): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "    (9): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "[Network netD] Total number of parameters : 11.034 M\n",
            "-----------------------------------------------\n",
            "features.0.res_ops.0.1.1.weight\n",
            "features.0.res_ops.1.1.1.weight\n",
            "features.0.res_ops.2.1.1.weight\n",
            "features.0.dw_ops.0.0.1.weight\n",
            "features.0.dw_ops.1.0.1.weight\n",
            "features.0.dw_ops.2.0.1.weight\n",
            "features.1.res_ops.0.1.1.weight\n",
            "features.1.res_ops.1.1.1.weight\n",
            "features.1.res_ops.2.1.1.weight\n",
            "features.1.dw_ops.0.0.1.weight\n",
            "features.1.dw_ops.1.0.1.weight\n",
            "features.1.dw_ops.2.0.1.weight\n",
            "features.2.res_ops.0.1.1.weight\n",
            "features.2.res_ops.1.1.1.weight\n",
            "features.2.res_ops.2.1.1.weight\n",
            "features.2.dw_ops.0.0.1.weight\n",
            "features.2.dw_ops.1.0.1.weight\n",
            "features.2.dw_ops.2.0.1.weight\n",
            "features.3.res_ops.0.1.1.weight\n",
            "features.3.res_ops.1.1.1.weight\n",
            "features.3.res_ops.2.1.1.weight\n",
            "features.3.dw_ops.0.0.1.weight\n",
            "features.3.dw_ops.1.0.1.weight\n",
            "features.3.dw_ops.2.0.1.weight\n",
            "features.4.res_ops.0.1.1.weight\n",
            "features.4.res_ops.1.1.1.weight\n",
            "features.4.res_ops.2.1.1.weight\n",
            "features.4.dw_ops.0.0.1.weight\n",
            "features.4.dw_ops.1.0.1.weight\n",
            "features.4.dw_ops.2.0.1.weight\n",
            "features.5.res_ops.0.1.1.weight\n",
            "features.5.res_ops.1.1.1.weight\n",
            "features.5.res_ops.2.1.1.weight\n",
            "features.5.dw_ops.0.0.1.weight\n",
            "features.5.dw_ops.1.0.1.weight\n",
            "features.5.dw_ops.2.0.1.weight\n",
            "features.6.res_ops.0.1.1.weight\n",
            "features.6.res_ops.1.1.1.weight\n",
            "features.6.res_ops.2.1.1.weight\n",
            "features.6.dw_ops.0.0.1.weight\n",
            "features.6.dw_ops.1.0.1.weight\n",
            "features.6.dw_ops.2.0.1.weight\n",
            "features.7.res_ops.0.1.1.weight\n",
            "features.7.res_ops.1.1.1.weight\n",
            "features.7.res_ops.2.1.1.weight\n",
            "features.7.dw_ops.0.0.1.weight\n",
            "features.7.dw_ops.1.0.1.weight\n",
            "features.7.dw_ops.2.0.1.weight\n",
            "features.8.res_ops.0.1.1.weight\n",
            "features.8.res_ops.1.1.1.weight\n",
            "features.8.res_ops.2.1.1.weight\n",
            "features.8.dw_ops.0.0.1.weight\n",
            "features.8.dw_ops.1.0.1.weight\n",
            "features.8.dw_ops.2.0.1.weight\n",
            "scale range: [0.30794793367385864, 1.434011697769165]\n",
            "scale threshold: 1.127203345298767, searched flops: 2594521088, target flops: 2600000000.0, flops diff: -5478912.0.\n",
            "All layers are pruned.\n",
            "---------- Networks initialized -------------\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ConstantPad2d(padding=(3, 3, 3, 3), value=0.0)\n",
            "    (1): Conv2d(3, 16, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(16, 22, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(22, 138, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): BatchNorm2d(138, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(138, 138, res_channels=[37, 0, 0], dw_channels=[6, 7, 0], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(138, 138, res_channels=[29, 0, 0], dw_channels=[2, 5, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(138, 138, res_channels=[34, 0, 0], dw_channels=[4, 5, 0], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(138, 138, res_channels=[26, 0, 0], dw_channels=[4, 4, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(138, 138, res_channels=[23, 0, 0], dw_channels=[3, 3, 2], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(138, 138, res_channels=[24, 0, 0], dw_channels=[5, 1, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(138, 138, res_channels=[27, 0, 0], dw_channels=[2, 1, 0], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(138, 138, res_channels=[19, 0, 0], dw_channels=[1, 0, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(138, 138, res_channels=[15, 0, 0], dw_channels=[3, 1, 1], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(138, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ConstantPad2d(padding=(3, 3, 3, 3), value=0.0)\n",
            "    (7): Conv2d(16, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_student] Total number of parameters : 0.149 M\n",
            "InceptionGenerator(\n",
            "  (down_sampling): Sequential(\n",
            "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (1): Conv2d(3, 64, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (9): ReLU(inplace=True)\n",
            "  )\n",
            "  (features): Sequential(\n",
            "    (0): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (1): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (2): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (3): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (4): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (5): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (6): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (7): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "    (8): InvertedResidualChannels(256, 256, res_channels=[42, 42, 42], dw_channels=[42, 42, 42], res_kernel_sizes=[1, 3, 5], dw_kernel_sizes=[1, 3, 5])\n",
            "  )\n",
            "  (up_sampling): Sequential(\n",
            "    (0): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
            "    (4): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
            "    (5): ReLU(inplace=True)\n",
            "    (6): ReflectionPad2d((3, 3, 3, 3))\n",
            "    (7): Conv2d(64, 3, kernel_size=(7, 7), stride=(1, 1))\n",
            "    (8): Tanh()\n",
            "  )\n",
            ")\n",
            "[Network netG_teacher] Total number of parameters : 8.154 M\n",
            "NLayerDiscriminator(\n",
            "  (model): Sequential(\n",
            "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (2): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (3): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (5): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
            "    (6): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (8): Conv2d(512, 1024, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "    (9): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
            "    (10): LeakyReLU(negative_slope=0.2, inplace=True)\n",
            "    (11): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
            "  )\n",
            ")\n",
            "[Network netD] Total number of parameters : 11.034 M\n",
            "-----------------------------------------------\n",
            "netG teacher FLOPs: 87054811136.\n",
            "netG student FLOPs: 2574811136.\n",
            "netG teacher FLOPs: 87054811136; Params: 8154255.\n",
            "netG student FLOPs: 2594521088; Params: 148681.\n",
            "Onnx exporting finished!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add Preprocssing and Postprocessing to ONNX model\n",
        "\n",
        "By default, our model is trained with inputs and outputs in the range of [-1,1]. However when importing into Lens Studio, we expect our inputs/outputs consists of RGB values in the range of [0, 255]. Here, we demonstrates how we can alter our ONNX model to normalize our inputs and scale outputs back for Lens Studio import."
      ],
      "metadata": {
        "id": "8_pNeRzPNieL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "import compose # compose.py\n",
        "from onnx import helper, checker\n",
        "\n",
        "def make_linear_model(alpha, beta, input_shape, graph_name, input_name, output_name,):\n",
        "    inputs = [helper.make_tensor_value_info(input_name, onnx.TensorProto.FLOAT, shape=input_shape)]\n",
        "    outputs = [helper.make_tensor_value_info(output_name, onnx.TensorProto.FLOAT, shape=input_shape)]\n",
        "    \n",
        "    # X = X * alpha\n",
        "    scale_name = f\"{graph_name}:scale\"\n",
        "    scale_output_name = f\"{graph_name}:scaled\"\n",
        "    scale_tensor = helper.make_tensor(scale_name, onnx.TensorProto.FLOAT, [1], [alpha])\n",
        "    scale = helper.make_node('Constant', [], [scale_name], scale_name, value=scale_tensor)\n",
        "    mul = helper.make_node('Mul',inputs=[input_name, scale_name], outputs=[scale_output_name], name=scale_output_name)\n",
        "    \n",
        "    # X = X + beta\n",
        "    bias_name = f\"{graph_name}:shift\"\n",
        "    bias_output_name = f\"{graph_name}:shifted\"\n",
        "    bias_tensor = helper.make_tensor(bias_name, onnx.TensorProto.FLOAT, [1], [beta])\n",
        "    bias = helper.make_node('Constant', [], [bias_name], bias_name, value=bias_tensor)\n",
        "    add = helper.make_node('Add', inputs=[scale_output_name, bias_name], outputs=[output_name], name=bias_output_name)\n",
        "    \n",
        "    # Create graph and model\n",
        "    graph = helper.make_graph([scale, mul, bias, add], graph_name, inputs, outputs)\n",
        "    model = helper.make_model(graph, producer_name='snapml-demo')\n",
        "    \n",
        "    # align with 2p6BnetG_student's IR and Opset version\n",
        "    model.ir_version = 6\n",
        "    model.opset_import[0].version = 11\n",
        "\n",
        "    # validate model\n",
        "    checker.check_model(model)\n",
        "    return model\n",
        "\n",
        "def scale_model_for_studio(original_model_path, new_model_path):\n",
        "    original_model = onnx.load(original_model_path) \n",
        "    input_dim = original_model.graph.input[0].type.tensor_type.shape.dim\n",
        "    input_shape = ['None'] + [item.dim_value for item in input_dim[1:]]\n",
        "\n",
        "    # add preprocessing to normalize inputs from [0,255] to [-1,1]\n",
        "    preprocessing_model = make_linear_model(2/255, -1, input_shape, 'preprocessing', 'raw_input', 'input')\n",
        "    combined_model = compose.merge_models(\n",
        "        preprocessing_model, original_model,\n",
        "        io_map=[('input', 'input')]\n",
        "    )\n",
        "\n",
        "    # add postprocessing to transform outputs back to [0,255]\n",
        "    postprocessing_model = make_linear_model(255/2, 255/2, input_shape, 'postprocessing', 'output', 'raw_output')\n",
        "    combined_model = compose.merge_models(\n",
        "        combined_model, postprocessing_model,\n",
        "        io_map=[('output', 'output')]\n",
        "    )\n",
        "    \n",
        "    # validate model\n",
        "    checker.check_model(combined_model)\n",
        "\n",
        "    # save scaled model\n",
        "    print(f\"Saving scaled model to {new_model_path}\")\n",
        "    onnx.save(combined_model, new_model_path)"
      ],
      "metadata": {
        "id": "_mjQ7zj-Nhtf"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMSdAQIU3gEf"
      },
      "source": [
        "### Download onnx file to local"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoePA3Y33jhq"
      },
      "source": [
        "def download_onnx_from_colab(path):\n",
        "    from google.colab import files\n",
        "    files.download(path)\n",
        "\n",
        "ONNX_PATH = 'onnx_files/cycle_gan/ukiyoe2photo/inception/student/2p6BnetG_student.onnx'\n",
        "SCALED_ONNX_PATH = ONNX_PATH.replace('.onnx', '_scaled.onnx')\n",
        "scale_model_for_studio(ONNX_PATH, SCALED_ONNX_PATH)\n",
        "download_onnx_from_colab(SCALED_ONNX_PATH)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSzjbFRevbvY"
      },
      "source": [
        "### Pre-trained student model\n",
        "\n",
        "We also provide pre-trained student models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2nvV0T6GH3D",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "af0da747-6aed-4746-c9a3-9ee989893a7a"
      },
      "source": [
        "# download 2.6B FLOPs ukyio student onnx model\n",
        "!gdown --id 1hxQi5N9Ft8nhMm5ckVFQvlWm2zWrOUad\n",
        "# download to local\n",
        "ONNX_PATH = 'ukyio-2p6BnetG_student.onnx'\n",
        "SCALED_ONNX_PATH = ONNX_PATH.replace('.onnx', '_scaled.onnx')\n",
        "scale_model_for_studio(ONNX_PATH, SCALED_ONNX_PATH)\n",
        "download_onnx_from_colab(SCALED_ONNX_PATH)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hxQi5N9Ft8nhMm5ckVFQvlWm2zWrOUad\n",
            "To: /content/CAT/ukyio-2p6BnetG_student.onnx\n",
            "\r  0% 0.00/667k [00:00<?, ?B/s]\r100% 667k/667k [00:00<00:00, 95.6MB/s]\n",
            "Saving scaled model to ukyio-2p6BnetG_student_scaled.onnx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_8398a24f-5d57-4bac-99aa-f9e8358734c0\", \"ukyio-2p6BnetG_student_scaled.onnx\", 668410)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43Tg67JI4zeE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "c51ffb31-36fc-48cf-a1b7-603e89b995bd"
      },
      "source": [
        "# download 5.2B FLOPs ukyio student onnx model\n",
        "!gdown --id 1L4iq5KrhtrUZ9tYzD-4pOl_nl1k4cOwt\n",
        "# download to local\n",
        "ONNX_PATH = 'ukyio-5p2BnetG_student.onnx'\n",
        "SCALED_ONNX_PATH = ONNX_PATH.replace('.onnx', '_scaled.onnx')\n",
        "scale_model_for_studio(ONNX_PATH, SCALED_ONNX_PATH)\n",
        "download_onnx_from_colab(SCALED_ONNX_PATH)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1L4iq5KrhtrUZ9tYzD-4pOl_nl1k4cOwt\n",
            "To: /content/CAT/ukyio-5p2BnetG_student.onnx\n",
            "\r  0% 0.00/1.72M [00:00<?, ?B/s]\r100% 1.72M/1.72M [00:00<00:00, 115MB/s]\n",
            "Saving scaled model to ukyio-5p2BnetG_student_scaled.onnx\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_85bc1f7c-7e47-4f14-bb47-c98946f2d290\", \"ukyio-5p2BnetG_student_scaled.onnx\", 1716177)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
